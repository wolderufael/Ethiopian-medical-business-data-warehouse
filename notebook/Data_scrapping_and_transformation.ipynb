{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data scraping and collection pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding project path to system path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "notebook_dir = os.getcwd()\n",
    "parent_path=os.path.dirname(notebook_dir)\n",
    "\n",
    "os.chdir(parent_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from script.telegram_scrapper import Scrapper\n",
    "from script.data_cleaning_and_transformation import Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantce of the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapper = Scrapper()\n",
    "transormer=Transformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scrapping\n",
    " Scraping data from Telegram channels involves using a custom script to collect messages, user interactions, and other relevant information from public or private channels. This process typically requires the use of Telegram's API or libraries that facilitate communication with Telegram.Steps to follow:\n",
    " * **Install Required Libraries**: You’ll need libraries such as telethon for interacting with Telegram’s API.\n",
    " * **Create a Telegram Application**: Visit the Telegram API development page to create a new application. You will get an `API ID` and `API hash`, which are essential for authenticating your requests.\n",
    " * **Authenticate**: Use your phone number to authenticate your script. This usually involves sending a code to your Telegram app.\n",
    " * **Run the Script**: Run your script using an asynchronous event loop.\n",
    " * **Store Data**: Save the scraped data to a CSV file for further analysis or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "await scrapper.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Loading to data base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once data is collected, it will undergo a rigorous cleaning and transformation process to ensure its accuracy and usability. Key steps include:\n",
    "\n",
    "* **Duplicate Removal**: Identifying and eliminating duplicate entries to avoid data redundancy.\n",
    "\n",
    "* **Missing Value Handling**: Addressing missing information through imputation methods or data exclusion, depending on the context.\n",
    "\n",
    "* **Standardization**: Ensuring consistent formatting for all data points (e.g., date format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame successfully added to table 'scrapped_data_cleaned' in the database.\n",
      "SQLAlchemy engine is disposed.\n"
     ]
    }
   ],
   "source": [
    "transormer.clean_and_load_to_db(\"data\",\"scrapped_data_cleaned\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
